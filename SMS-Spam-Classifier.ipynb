{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T12:47:22.297638Z",
     "start_time": "2020-06-25T12:47:22.291654Z"
    }
   },
   "source": [
    "<b>`findspark` is a  python library which is used to find the location of the Spark installed on the machine.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T06:27:28.589362Z",
     "start_time": "2020-06-27T06:27:28.582413Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T06:27:32.995303Z",
     "start_time": "2020-06-27T06:27:28.591358Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .master('local[*]') \\\n",
    "                    .appName('sms') \\\n",
    "                    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>the master node 'local[*]' tells Spark to run locally with as many worker threads as logical cores on your machine.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T06:27:34.455096Z",
     "start_time": "2020-06-27T06:27:32.997289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"text\", StringType()),\n",
    "    StructField(\"label\", IntegerType())\n",
    "])\n",
    "\n",
    "sms = spark.read.csv(\"sms.csv\", sep=';', header=False, schema=schema)\n",
    "\n",
    "sms.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T13:07:20.689440Z",
     "start_time": "2020-06-25T13:07:20.684455Z"
    }
   },
   "source": [
    "<b>The data has no headers therefore I explicitly define the schema</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T06:27:35.984275Z",
     "start_time": "2020-06-27T06:27:34.458086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+------+\n",
      "| id|                text|label|length|\n",
      "+---+--------------------+-----+------+\n",
      "|  1|Sorry, I'll call ...|    0|    33|\n",
      "|  2|Dont worry. I gue...|    0|    30|\n",
      "|  3|Call FREEPHONE 08...|    1|    33|\n",
      "|  4|Win a 1000 cash p...|    1|    43|\n",
      "|  5|Go until jurong p...|    0|   111|\n",
      "|  6|Ok lar... Joking ...|    0|    29|\n",
      "|  7|Free entry in 2 a...|    1|   155|\n",
      "|  8|U dun say so earl...|    0|    49|\n",
      "|  9|Nah I don't think...|    0|    61|\n",
      "| 10|FreeMsg Hey there...|    1|   146|\n",
      "| 11|Even my brother i...|    0|    77|\n",
      "| 12|As per your reque...|    0|   158|\n",
      "| 13|WINNER!! As a val...|    1|   156|\n",
      "| 14|Had your mobile 1...|    1|   154|\n",
      "| 15|I'm gonna be home...|    0|   109|\n",
      "| 16|SIX chances to wi...|    1|   136|\n",
      "| 17|URGENT! You have ...|    1|   154|\n",
      "| 18|I've been searchi...|    0|   196|\n",
      "| 19|I HAVE A DATE ON ...|    0|    35|\n",
      "| 20|XXXMobileMovieClu...|    1|   149|\n",
      "+---+--------------------+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length\n",
    "sms = sms.withColumn('length', length(sms['text']))\n",
    "sms.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T13:08:58.876488Z",
     "start_time": "2020-06-25T13:08:58.869512Z"
    }
   },
   "source": [
    "<b>Next, I create a new column 'length' which signifies the length of the SMS.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T06:27:36.694375Z",
     "start_time": "2020-06-27T06:27:35.986270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|label|       avg(length)|\n",
      "+-----+------------------+\n",
      "|    0| 70.70188522892066|\n",
      "|    1|138.14457831325302|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sms.groupBy('label').avg('length').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>This is interesting, spam messages are twice as long as regular messages.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T06:27:36.712327Z",
     "start_time": "2020-06-27T06:27:36.697369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- length: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sms.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T06:27:36.925755Z",
     "start_time": "2020-06-27T06:27:36.717313Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "wrangled = sms.withColumn('text', regexp_replace(sms.text, '[_():;,.!?\\\\-]', ' '))\n",
    "\n",
    "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, '[0-9]', ' '))\n",
    "\n",
    "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, ' +', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Above, we remove anything other that letters (eg- punctuations,numbers and symbols)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T06:27:37.262854Z",
     "start_time": "2020-06-27T06:27:36.927750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0| 4827|\n",
      "|    1|  747|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wrangled.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>There are a total of 5574 SMS, of which only 747 have been labelled as spam.This dataset is highly imbalanced.As a classifier just predicting all the messages as not spam will get a accuracy of 87%.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T06:27:37.444368Z",
     "start_time": "2020-06-27T06:27:37.264848Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+------+\n",
      "| id|                text|label|length|\n",
      "+---+--------------------+-----+------+\n",
      "|  1|Sorry I'll call l...|    0|    33|\n",
      "|  2|Dont worry I gues...|    0|    30|\n",
      "|  3| Call FREEPHONE now |    1|    33|\n",
      "|  4|Win a cash prize ...|    1|    43|\n",
      "|  5|Go until jurong p...|    0|   111|\n",
      "|  6|Ok lar Joking wif...|    0|    29|\n",
      "|  7|Free entry in a w...|    1|   155|\n",
      "|  8|U dun say so earl...|    0|    49|\n",
      "|  9|Nah I don't think...|    0|    61|\n",
      "| 10|FreeMsg Hey there...|    1|   146|\n",
      "| 11|Even my brother i...|    0|    77|\n",
      "| 12|As per your reque...|    0|   158|\n",
      "| 13|WINNER As a value...|    1|   156|\n",
      "| 14|Had your mobile m...|    1|   154|\n",
      "| 15|I'm gonna be home...|    0|   109|\n",
      "| 16|SIX chances to wi...|    1|   136|\n",
      "| 17|URGENT You have w...|    1|   154|\n",
      "| 18|I've been searchi...|    0|   196|\n",
      "| 19|I HAVE A DATE ON ...|    0|    35|\n",
      "| 20|XXXMobileMovieClu...|    1|   149|\n",
      "+---+--------------------+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wrangled.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T06:27:37.709659Z",
     "start_time": "2020-06-27T06:27:37.446363Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
    "\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='terms')\n",
    "\n",
    "hasher = HashingTF(inputCol=remover.getOutputCol(), outputCol=\"hash\")\n",
    "\n",
    "idf = IDF(inputCol=hasher.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hasher, idf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>First we tokenize the text into individual tokens,then remove stopwords.After that I perform hashing(hashing provides a fast and space-efficient way to map a huge number of words present in the SMS messages onto a smaller, finite number of values.At last I create a TF-IDF matrix which gives relatively higher importance to words that are rare across documents.<br>\n",
    "Next, I create a pipeline which wraps all of the above steps. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T06:27:39.482878Z",
     "start_time": "2020-06-27T06:27:37.712653Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline_model = pipeline.fit(wrangled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T06:27:39.593564Z",
     "start_time": "2020-06-27T06:27:39.485855Z"
    }
   },
   "outputs": [],
   "source": [
    "sms_transformed = pipeline_model.transform(wrangled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T06:27:39.630465Z",
     "start_time": "2020-06-27T06:27:39.595562Z"
    }
   },
   "outputs": [],
   "source": [
    "sms_train, sms_test = sms_transformed.randomSplit([0.8, 0.2], seed=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Weights (Handling Imbalanced Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Since where we have 87% positives (label == 0) in the dataset, so theoretically we want to \"under-sample\" the positive class. So that The logistic loss objective function should treat the negative class (label == 1) with higher weight.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T06:27:42.324010Z",
     "start_time": "2020-06-27T06:27:39.631465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of ones are 601\n",
      "Percentage of ones are 13.421170165252343\n"
     ]
    }
   ],
   "source": [
    "dataset_size=float(sms_train.select(\"label\").count())\n",
    "numPositives=sms_train.select(\"label\").where('label == 1').count()\n",
    "per_ones=(float(numPositives)/float(dataset_size))*100\n",
    "numNegatives=float(dataset_size-numPositives)\n",
    "print('The number of ones are {}'.format(numPositives))\n",
    "print('Percentage of ones are {}'.format(per_ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T06:27:42.334348Z",
     "start_time": "2020-06-27T06:27:42.326989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BalancingRatio = 0.8657882983474765\n"
     ]
    }
   ],
   "source": [
    "BalancingRatio= numNegatives/dataset_size\n",
    "print('BalancingRatio = {}'.format(BalancingRatio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T06:27:43.329307Z",
     "start_time": "2020-06-27T06:27:42.337959Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+--------------------+\n",
      "|      classWeights|label|            features|\n",
      "+------------------+-----+--------------------+\n",
      "|0.1342117016525235|    0|(262144,[78820,10...|\n",
      "|0.8657882983474765|    1|(262144,[46213,10...|\n",
      "|0.8657882983474765|    1|(262144,[51247,77...|\n",
      "|0.1342117016525235|    0|(262144,[38555,52...|\n",
      "|0.8657882983474765|    1|(262144,[29259,68...|\n",
      "+------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "sms_train=sms_train.withColumn(\"classWeights\", when(sms_train.label == 1,BalancingRatio).otherwise(1-BalancingRatio))\n",
    "sms_train.select(\"classWeights\",\"label\",\"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T04:31:09.621735Z",
     "start_time": "2020-06-27T04:31:09.616746Z"
    }
   },
   "source": [
    "<b>Here we give a weight of ~0.87 to spam messages and ~0.13 to non spam messages.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T06:27:48.225206Z",
     "start_time": "2020-06-27T06:27:43.334291Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\",weightCol=\"classWeights\",maxIter=10)\n",
    "lr_model = lr.fit(sms_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T06:27:50.003450Z",
     "start_time": "2020-06-27T06:27:48.227203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under ROC for test set 0.9720259552992109\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "prediction = lr_model.transform(sms_test)\n",
    "print(\"The area under ROC for test set {}\".format(evaluator.evaluate(prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Wow! we get a roc-auc score of 97% with our baseline model.But it is a fact that F1 score is a better evaluation metric than roc-auc when dealing with imbalanced datasets (see [here](https://www.kaggle.com/lct14558/imbalanced-data-why-you-should-not-use-roc-curve)),so we will consider F1 score as well.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T06:27:51.326909Z",
     "start_time": "2020-06-27T06:27:50.008439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------------------------------------------+\n",
      "|label|prediction|probability                               |\n",
      "+-----+----------+------------------------------------------+\n",
      "|0    |0.0       |[0.9952162043831404,0.004783795616859571] |\n",
      "|0    |0.0       |[0.9999853417220804,1.4658277919459315E-5]|\n",
      "|1    |1.0       |[1.2911360078789951E-5,0.9999870886399211]|\n",
      "|1    |0.0       |[0.9769451956618017,0.023054804338198266] |\n",
      "|0    |0.0       |[0.9596079884065271,0.04039201159347307]  |\n",
      "+-----+----------+------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.select('label', 'prediction', 'probability').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T06:27:52.233482Z",
     "start_time": "2020-06-27T06:27:51.329904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    0|       0.0|  942|\n",
      "|    1|       1.0|  126|\n",
      "|    1|       0.0|   20|\n",
      "|    0|       1.0|    8|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>F1 score is basically the harmonic mean of precision and recall, the harmonic mean gives much more weight to low values.As a result, the classifier will only get a high F1 score if both recall and precision are high.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T06:27:56.422275Z",
     "start_time": "2020-06-27T06:27:52.239467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 0.94\n",
      "recall    = 0.86\n",
      "F1 Score  = 0.90\n"
     ]
    }
   ],
   "source": [
    "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = prediction.filter('prediction = 0 AND label != prediction').count()\n",
    "FP = prediction.filter('prediction = 1 AND label != prediction').count()\n",
    "\n",
    "precision = TP/(TP+FP)\n",
    "recall = TP/(TP+FN)\n",
    "F1score = (2*precision*recall)/(precision+recall)\n",
    "print('precision = {:.2f}\\nrecall    = {:.2f}\\nF1 Score  = {:.2f}'.format(precision, recall,F1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>As you can see we achieved a roc-auc score of 97% but a F1 score of just 90%.With hyper parameter using Grid search I think we can achieve much better results!</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T06:27:56.437235Z",
     "start_time": "2020-06-27T06:27:56.426265Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "params = ParamGridBuilder().addGrid(hasher.numFeatures, [1024, 4096, 16384]) \\\n",
    "                           .addGrid(hasher.binary, [True, False]) \\\n",
    "                           .addGrid(lr.regParam, [0.01, 0.1, 1.0, 10.0]) \\\n",
    "                           .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "                           .build()\n",
    "\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=params,evaluator = evaluator, numFolds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T05:21:49.618218Z",
     "start_time": "2020-06-27T05:21:49.605254Z"
    }
   },
   "source": [
    "<b>There are 72 points in the parameter grid and 5 folds in the cross-validator.Therefore 360 models are built,it took me 11 minutes for training.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T06:38:57.872638Z",
     "start_time": "2020-06-27T06:27:56.442225Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_cv_model = cv.fit(sms_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T06:38:58.569772Z",
     "start_time": "2020-06-27T06:38:57.875630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under ROC for test set after cv 0.9843186733958208\n"
     ]
    }
   ],
   "source": [
    "prediction_cv = lr_cv_model.transform(sms_test)\n",
    "print(\"The area under ROC for test set after cv {}\".format(evaluator.evaluate(prediction_cv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T06:39:01.801127Z",
     "start_time": "2020-06-27T06:38:58.571769Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 0.98\n",
      "recall    = 0.88\n",
      "F1 Score  = 0.93\n"
     ]
    }
   ],
   "source": [
    "TN = prediction_cv.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = prediction_cv.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = prediction_cv.filter('prediction = 0 AND label != prediction').count()\n",
    "FP = prediction_cv.filter('prediction = 1 AND label != prediction').count()\n",
    "\n",
    "precision = TP/(TP+FP)\n",
    "recall = TP/(TP+FN)\n",
    "F1score = (2*precision*recall)/(precision+recall)\n",
    "print('precision = {:.2f}\\nrecall    = {:.2f}\\nF1 Score  = {:.2f}'.format(precision, recall,F1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>To conclude after all the hyper parameter tuning we end up witha model with a roc-auc score of 98% and F1 score of 93%.<br>It is worth mentioning that without adding the class weights we will end up with a model with an F1 score of 0 as precision was 0 but a roc-auc score of nearly 100%.This is because of the highly imbalanced data the model blindly predicts all the messages to be not spam.So it is very important to address the problem of imbalanced datasets.  </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
